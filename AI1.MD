# TRÍ TUỆ NHÂN TẠO
---

## GIỚI THIỆU

Phép thử Turing (Turing Test), do Alan Turing (1950) đề xuất, được thiết kế để cung cấp một định nghĩa hoạt động thỏa đáng về trí thông minh. Một máy tính vượt qua phép thử nếu một người thẩm vấn, sau khi đặt một số câu hỏi bằng văn bản, không thể phân biệt được liệu các câu trả lời bằng văn bản đến từ một người hay từ một máy tính. Chương 26 thảo luận chi tiết về phép thử này và liệu một máy tính có thực sự thông minh nếu nó vượt qua. Hiện tại, chúng ta lưu ý rằng việc lập trình một máy tính để vượt qua một phép thử được áp dụng nghiêm ngặt đòi hỏi rất nhiều công việc. Máy tính đó cần phải sở hữu các khả năng sau:

*   **Xử lý ngôn ngữ tự nhiên (Natural Language Processing)** để cho phép nó giao tiếp thành công bằng tiếng Anh;
*   **Biểu diễn tri thức (Knowledge Representation)** để lưu trữ những gì nó biết hoặc nghe được;
*   **Suy luận tự động (Automated Reasoning)** để sử dụng thông tin đã lưu trữ nhằm trả lời câu hỏi và rút ra kết luận mới;
*   **Học máy (Machine Learning)** để thích ứng với hoàn cảnh mới và để phát hiện cũng như ngoại suy các mẫu hình.

---

## NỀN TẢNG CỦA TRÍ TUỆ NHÂN TẠO (AI)

Trong phần này, chúng tôi cung cấp một lịch sử tóm tắt về các ngành học đã đóng góp ý tưởng, quan điểm và kỹ thuật cho AI. Giống như bất kỳ lịch sử nào, lịch sử này buộc phải tập trung vào một số ít người, sự kiện và ý tưởng, đồng thời bỏ qua những yếu tố quan trọng khác. Chúng tôi tổ chức lịch sử xoay quanh một loạt câu hỏi. Chúng tôi chắc chắn không muốn tạo ấn tượng rằng những câu hỏi này là những câu hỏi duy nhất mà các ngành học giải quyết hoặc tất cả các ngành học đều hướng tới AI như là thành quả cuối cùng của chúng.

### 1.2.1 Triết học

*   Liệu các quy tắc hình thức có thể được sử dụng để rút ra kết luận hợp lệ không?
*   Tâm trí phát sinh từ một bộ não vật lý như thế nào?
*   Tri thức đến từ đâu?
*   Tri thức dẫn đến hành động như thế nào?

Aristotle (384-322 TCN), người có tượng bán thân xuất hiện trên bìa trước của cuốn sách này, là người đầu tiên xây dựng một tập hợp các quy luật chính xác chi phối phần lý trí của tâm trí. Ông đã phát triển một hệ thống tam đoạn luận không chính thức cho việc suy luận đúng đắn, về nguyên tắc cho phép một người tạo ra kết luận một cách máy móc, dựa trên các tiền đề ban đầu. Rất lâu sau, Ramon Lull (mất năm 1315) đã có ý tưởng rằng suy luận hữu ích thực sự có thể được thực hiện bởi một tạo tác cơ học. Thomas Hobbes (1588-1679) cho rằng suy luận giống như tính toán số học, rằng "chúng ta cộng và trừ trong những suy nghĩ thầm lặng của mình." Bản thân việc tự động hóa tính toán đã được tiến hành tốt đẹp. Khoảng năm 1500, Leonardo da Vinci (1452-1519) đã thiết kế nhưng không chế tạo một máy tính cơ học; các bản tái tạo gần đây đã cho thấy thiết kế này hoạt động được. Máy tính toán đầu tiên được biết đến được chế tạo vào khoảng năm 1623 bởi nhà khoa học người Đức Wilhelm Schickard (1592-1635), mặc dù Pascaline, được chế tạo vào năm 1642 bởi Blaise Pascal (1623-1662), (là máy tính được biết đến rộng rãi đầu tiên).

### 1.2.2 Toán học

*   Các quy tắc hình thức để rút ra kết luận hợp lệ là gì?
*   Điều gì có thể được tính toán?
*   Chúng ta suy luận với thông tin không chắc chắn như thế nào?

Các nhà triết học đã đặt nền móng cho một số ý tưởng cơ bản của AI, nhưng bước nhảy vọt để trở thành một khoa học hình thức đòi hỏi một mức độ hình thức hóa toán học trong ba lĩnh vực cơ bản: logic, tính toán và xác suất.

Ý tưởng về logic hình thức có thể bắt nguồn từ các nhà triết học Hy Lạp cổ đại, nhưng sự phát triển toán học của nó thực sự bắt đầu với công trình của George Boole (1815–1864), người đã xây dựng chi tiết về logic mệnh đề, hay logic Boole (Boole, 1847). Năm 1879, Gottlob Frege (1848-1925) đã mở rộng logic của Boole để bao gồm các đối tượng và quan hệ, tạo ra logic bậc nhất (first-order logic) được sử dụng ngày nay. Alfred Tarski (1902-1983) đã giới thiệu một lý thuyết tham chiếu cho thấy cách liên kết các đối tượng trong logic với các đối tượng trong thế giới thực.

Bước tiếp theo là xác định giới hạn của những gì có thể thực hiện được bằng logic và tính toán. Thuật toán (algorithm) phi tầm thường đầu tiên được cho là thuật toán của Euclid để tính ước số chung lớn nhất. Từ "algorithm" (và ý tưởng nghiên cứu chúng) bắt nguồn từ al-Khowarazmi, một nhà toán học Ba Tư thế kỷ thứ 9, người có các tác phẩm cũng giới thiệu chữ số Ả Rập và đại số cho châu Âu. Boole và những người khác đã thảo luận về các thuật toán cho suy luận logic, và vào cuối thế kỷ 19, các nỗ lực đã được tiến hành để hình thức hóa lý luận toán học tổng quát như là suy luận logic. Năm 1930, Kurt Gödel (1906–1978) đã chỉ ra rằng tồn tại một thủ tục hiệu quả để chứng minh bất kỳ mệnh đề đúng nào trong logic bậc nhất của Frege và Russell, nhưng logic bậc nhất không thể nắm bắt được nguyên lý quy nạp toán học cần thiết để mô tả các số tự nhiên. Năm 1931, Gödel đã chỉ ra rằng các giới hạn về suy luận thực sự tồn tại. Định lý bất toàn (incompleteness theorem) của ông cho thấy rằng trong bất kỳ lý thuyết hình thức nào đủ mạnh như số học Peano (lý thuyết cơ bản về số tự nhiên), đều có những mệnh đề đúng không thể quyết định được, theo nghĩa là chúng không có bằng chứng trong lý thuyết đó.

Kết quả cơ bản này cũng có thể được hiểu là cho thấy một số hàm trên các số nguyên không thể được biểu diễn bằng một thuật toán—tức là chúng không thể được tính toán. Điều này đã thúc đẩy Alan Turing (1912-1954) cố gắng mô tả chính xác những hàm nào có thể tính toán được (computable)—có khả năng được tính toán. Khái niệm này thực ra hơi có vấn đề vì khái niệm về một phép tính hoặc một thủ tục hiệu quả thực sự không thể được định nghĩa một cách hình thức. Tuy nhiên, luận đề Church-Turing (Church-Turing thesis), phát biểu rằng máy Turing (Turing machine) (Turing, 1936) có khả năng tính toán bất kỳ hàm nào có thể tính toán được, thường được chấp nhận là cung cấp một định nghĩa đủ. Turing cũng chỉ ra rằng có một số hàm mà không máy Turing nào có thể tính toán được. Ví dụ, không có máy nào có thể nói chung chung liệu một chương trình nhất định sẽ trả về một câu trả lời cho một đầu vào nhất định hay sẽ chạy mãi mãi.

Mặc dù tính quyết định (decidability) và tính toán được (computability) rất quan trọng đối với sự hiểu biết về tính toán, khái niệm về tính khả giải (tractability) thậm chí còn có tác động lớn hơn. Nói một cách gần đúng, một vấn đề được gọi là bất khả giải (intractable) nếu thời gian cần thiết để giải quyết các trường hợp của vấn đề tăng theo cấp số nhân với kích thước của các trường hợp đó. Sự phân biệt giữa tăng trưởng đa thức và tăng trưởng theo cấp số nhân về độ phức tạp lần đầu tiên được nhấn mạnh vào giữa những năm 1960 (Cobham, 1964; Edmonds, 1965). Điều này quan trọng bởi vì tăng trưởng theo cấp số nhân có nghĩa là ngay cả những trường hợp có kích thước vừa phải cũng không thể được giải quyết trong bất kỳ khoảng thời gian hợp lý nào. Do đó, người ta nên cố gắng chia vấn đề tổng thể của việc tạo ra hành vi thông minh thành các vấn đề con khả giải thay vì các vấn đề bất khả giải.

Làm thế nào để nhận ra một vấn đề bất khả giải? Lý thuyết về NP-đầy đủ (NP-completeness), do Steven Cook (1971) và Richard Karp (1972) tiên phong, cung cấp một phương pháp. Cook và Karp đã chỉ ra sự tồn tại của các lớp lớn các vấn đề tìm kiếm tổ hợp và suy luận chuẩn mực là NP-đầy đủ. Bất kỳ lớp vấn đề nào mà lớp các vấn đề NP-đầy đủ có thể được rút gọn về đều có khả năng là bất khả giải. (Mặc dù vẫn chưa được chứng minh rằng NP-đầy đủ).

Bên cạnh logic và tính toán, đóng góp lớn thứ ba của toán học cho AI là lý thuyết xác suất (probability). Nhà toán học người Ý Gerolamo Cardano (1501-1576) lần đầu tiên đóng khung ý tưởng về xác suất, mô tả nó dưới dạng các kết quả có thể có của các sự kiện cờ bạc. Năm 1654, Blaise Pascal (1623-1662), trong một lá thư gửi Pierre Fermat (1601-1665), đã chỉ ra cách dự đoán tương lai của một ván cờ bạc chưa kết thúc và ấn định các khoản thanh toán trung bình cho những người đánh bạc. Xác suất nhanh chóng trở thành một phần vô giá của tất cả các ngành khoa học định lượng, giúp giải quyết các phép đo không chắc chắn và các lý thuyết không hoàn chỉnh. James Bernoulli (1654–1705), Pierre Laplace (1749-1827), và những người khác đã phát triển lý thuyết này và giới thiệu các phương pháp thống kê mới. Thomas Bayes (1702-1761), người xuất hiện trên bìa trước của cuốn sách này, đã đề xuất một quy tắc để cập nhật xác suất dựa trên bằng chứng mới. Quy tắc Bayes (Bayes' rule) là nền tảng của hầu hết các phương pháp tiếp cận hiện đại đối với suy luận không chắc chắn trong các hệ thống AI.

### 1.2.3 Kinh tế học

*   Chúng ta nên đưa ra quyết định như thế nào để tối đa hóa lợi ích?
*   Chúng ta nên làm điều này như thế nào khi những người khác có thể không đồng tình?
*   Chúng ta nên làm điều này như thế nào khi lợi ích có thể ở rất xa trong tương lai?

Khoa học kinh tế bắt đầu vào năm 1776, khi nhà triết học người Scotland Adam Smith (1723-1790) xuất bản cuốn *An Inquiry into the Nature and Causes of the Wealth of Nations* (Tìm hiểu về bản chất và nguyên nhân của sự giàu có của các quốc gia). Trong khi người Hy Lạp cổ đại và những người khác đã có những đóng góp cho tư tưởng kinh tế, Smith là người đầu tiên coi nó như một khoa học, sử dụng ý tưởng rằng các nền kinh tế có thể được coi là bao gồm các tác nhân cá nhân tối đa hóa lợi ích kinh tế của riêng họ. Hầu hết mọi người nghĩ về kinh tế học là về tiền bạc, nhưng các nhà kinh tế học sẽ nói rằng họ thực sự đang nghiên cứu cách mọi người đưa ra lựa chọn dẫn đến kết quả ưa thích. Khi McDonald's bán một chiếc hamburger với giá một đô la, họ đang khẳng định rằng họ muốn đồng đô la đó hơn và hy vọng rằng khách hàng sẽ muốn chiếc hamburger hơn. Việc xử lý toán học các "kết quả ưa thích" hay hữu dụng (utility) lần đầu tiên được chính thức hóa bởi Léon Walras (phát âm là "Valrasse") (1834-1910) và được cải thiện bởi Frank Ramsey (1931) và sau đó bởi John von Neumann và Oskar Morgenstern trong cuốn sách *The Theory of Games and Economic Behavior* (Lý thuyết Trò chơi và Hành vi Kinh tế) (1944).

Lý thuyết quyết định (Decision theory), kết hợp lý thuyết xác suất với lý thuyết hữu dụng, cung cấp một khuôn khổ chính thức và đầy đủ cho các quyết định (kinh tế hoặc khác) được đưa ra trong điều kiện không chắc chắn—tức là, trong các trường hợp mà các mô tả xác suất nắm bắt một cách thích hợp môi trường của người ra quyết định. Điều này phù hợp với các nền kinh tế "lớn", nơi mỗi tác nhân không cần chú ý đến hành động của các tác nhân khác với tư cách cá nhân. Đối với các nền kinh tế "nhỏ", tình hình giống như một trò chơi hơn: hành động của một người chơi có thể ảnh hưởng đáng kể đến hữu dụng của người khác (theo hướng tích cực hoặc tiêu cực). Sự phát triển lý thuyết trò chơi (game theory) của Von Neumann và Morgenstern (xem thêm Luce và Raiffa, 1957) bao gồm kết quả đáng ngạc nhiên rằng, đối với một số trò chơi, một tác nhân hợp lý nên áp dụng các chính sách được ngẫu nhiên hóa (hoặc ít nhất có vẻ như vậy). Không giống như lý thuyết quyết định, lý thuyết trò chơi không đưa ra một quy tắc rõ ràng để lựa chọn hành động.

Phần lớn, các nhà kinh tế học không giải quyết câu hỏi thứ ba được liệt kê ở trên, cụ thể là, làm thế nào để đưa ra quyết định hợp lý khi lợi ích từ các hành động không phải là ngay lập tức mà thay vào đó là kết quả của một số hành động được thực hiện theo trình tự. Chủ đề này được theo đuổi trong lĩnh vực nghiên cứu vận hành (operations research), xuất hiện trong Thế chiến II từ những nỗ lực ở Anh nhằm tối ưu hóa việc lắp đặt radar, và sau đó tìm thấy các ứng dụng dân sự trong các quyết định quản lý phức tạp. Công trình của Richard Bellman (1957) đã chính thức hóa một lớp các vấn đề quyết định tuần tự được gọi là quy trình quyết định Markov (Markov decision processes), mà chúng ta nghiên cứu trong Chương 17 và 21.

Công trình trong kinh tế học và nghiên cứu vận hành đã đóng góp nhiều vào khái niệm của chúng ta về các tác nhân hợp lý, tuy nhiên trong nhiều năm, nghiên cứu AI đã phát triển theo những con đường hoàn toàn riêng biệt. Một lý do là sự phức tạp rõ ràng của việc đưa ra quyết định hợp lý. Nhà nghiên cứu AI tiên phong Herbert Simon (1916–2001) đã đoạt giải Nobel Kinh tế năm 1978 cho công trình ban đầu của ông cho thấy các mô hình dựa trên sự thỏa mãn (satisficing)—đưa ra quyết định "đủ tốt,"

### 1.2.4 Khoa học thần kinh (Neuroscience)

*   Bộ não xử lý thông tin như thế nào?

Khoa học thần kinh là nghiên cứu về hệ thần kinh, đặc biệt là não. Mặc dù cách chính xác mà bộ não cho phép tư duy là một trong những bí ẩn lớn của khoa học, thực tế là nó *có* cho phép tư duy đã được đánh giá cao trong hàng ngàn năm vì bằng chứng cho thấy những cú đánh mạnh vào đầu có thể dẫn đến mất khả năng tâm thần. Từ lâu người ta cũng biết rằng bộ não con người có một số điểm khác biệt; vào khoảng năm 335 TCN, Aristotle đã viết: "Trong tất cả các loài động vật, con người có bộ não lớn nhất so với kích thước của nó." Tuy nhiên, phải đến giữa thế kỷ 18, bộ não mới được công nhận rộng rãi là nơi chứa đựng ý thức. Trước đó, các vị trí ứng cử viên bao gồm tim và lá lách.

Nghiên cứu của Paul Broca (1824-1880) về chứng mất ngôn ngữ (aphasia - thiếu hụt khả năng nói) ở những bệnh nhân bị tổn thương não vào năm 1861 đã chứng minh sự tồn tại của các vùng não cục bộ chịu trách nhiệm cho các chức năng nhận thức cụ thể. Đặc biệt, ông đã chỉ ra rằng việc sản xuất lời nói được cục bộ hóa ở phần bán cầu não trái mà ngày nay được gọi là vùng Broca. Vào thời điểm đó, người ta đã biết rằng não bao gồm các tế bào thần kinh, hay neuron, nhưng phải đến năm 1873, Camillo Golgi (1843-1926) mới phát triển một kỹ thuật nhuộm cho phép quan sát các neuron riêng lẻ trong não (xem Hình 1.2). Kỹ thuật này được Santiago Ramon y Cajal (1852-1934) sử dụng trong các nghiên cứu tiên phong của ông về cấu trúc neuron của não. Nicolas Rashevsky (1936, 1938) là người đầu tiên áp dụng các mô hình toán học vào nghiên cứu hệ thần kinh.

**Hình 1.2 Các bộ phận của một tế bào thần kinh hay neuron.** Mỗi neuron bao gồm một thân tế bào (soma), chứa nhân tế bào. Nhánh ra từ thân tế bào là một số sợi gọi là tua gai (dendrites) và một sợi dài duy nhất gọi là sợi trục (axon). Sợi trục kéo dài một khoảng cách lớn, dài hơn nhiều so với tỷ lệ trong sơ đồ này. Thông thường, một sợi trục dài 1 cm (gấp 100 lần đường kính của thân tế bào), nhưng có thể dài tới 1 mét. Một neuron tạo kết nối với 10 đến 100.000 neuron khác tại các điểm nối gọi là khớp thần kinh (synapses). Tín hiệu được truyền từ neuron này sang neuron khác bằng một phản ứng điện hóa phức tạp. Các tín hiệu này kiểm soát hoạt động của não trong ngắn hạn và cũng cho phép những thay đổi dài hạn trong kết nối của các neuron. Những cơ chế này được cho là tạo cơ sở cho việc học tập trong não. Hầu hết quá trình xử lý thông tin diễn ra ở vỏ não (cerebral cortex), lớp ngoài của não. Đơn vị tổ chức cơ bản dường như là một cột mô có đường kính khoảng 0,5 mm, chứa khoảng 20.000 neuron và kéo dài toàn bộ chiều sâu của vỏ não (khoảng 4 mm ở người).
*(Chú thích hình ảnh được dịch và tóm tắt ở trên)*

Hiện nay chúng ta có một số dữ liệu về việc lập bản đồ giữa các vùng của não và các bộ phận của cơ thể mà chúng kiểm soát hoặc từ đó chúng nhận đầu vào cảm giác. Các bản đồ như vậy có thể thay đổi hoàn toàn trong vài tuần, và một số động vật dường như có nhiều bản đồ. Hơn nữa, chúng ta không hoàn toàn hiểu làm thế nào các khu vực khác có thể đảm nhận chức năng khi một khu vực bị tổn thương. Hầu như không có lý thuyết nào về cách một ký ức cá nhân được lưu trữ.

Việc đo lường hoạt động của não nguyên vẹn bắt đầu vào năm 1929 với phát minh của Hans Berger về điện não đồ (EEG - electroencephalograph). Sự phát triển gần đây của hình ảnh cộng hưởng từ chức năng (fMRI - functional magnetic resonance imaging) (Ogawa et al., 1990; Cabeza và Nyberg, 2001) đang mang lại cho các nhà khoa học thần kinh những hình ảnh chi tiết chưa từng có về hoạt động của não, cho phép các phép đo tương ứng theo những cách thú vị với các quá trình nhận thức đang diễn ra. Những điều này được tăng cường bởi những tiến bộ trong ghi âm hoạt động của neuron đơn lẻ. Các neuron riêng lẻ có thể được kích thích bằng điện, hóa học, hoặc thậm chí quang học (Han và Boyden, 2007), cho phép lập bản đồ các mối quan hệ đầu vào-đầu ra của neuron. Bất chấp những tiến bộ này, chúng ta vẫn còn một chặng đường dài để hiểu các quá trình nhận thức thực sự hoạt động như thế nào.

### 1.2.5 Tâm lý học (Psychology)

*   Con người và động vật suy nghĩ và hành động như thế nào?

Nguồn gốc của tâm lý học khoa học thường được truy tìm từ công trình của nhà vật lý người Đức Hermann von Helmholtz (1821-1894) và học trò của ông là Wilhelm Wundt (1832-1920). Helmholtz đã áp dụng phương pháp khoa học vào nghiên cứu thị giác của con người, và cuốn *Handbook of Physiological Optics* (Sổ tay Quang học Sinh lý) của ông thậm chí ngày nay vẫn được mô tả là "luận án quan trọng nhất về vật lý và sinh lý của thị giác con người" (Nalwa, 1993, tr.15). Năm 1879, Wundt đã mở phòng thí nghiệm tâm lý học thực nghiệm đầu tiên tại Đại học Leipzig. Wundt nhấn mạnh vào các thí nghiệm được kiểm soát cẩn thận, trong đó các cộng sự của ông sẽ thực hiện một nhiệm vụ tri giác hoặc liên tưởng trong khi tự xem xét các quá trình suy nghĩ của họ. Việc kiểm soát cẩn thận đã góp phần lớn vào việc đưa tâm lý học trở thành một khoa học, nhưng bản chất chủ quan của dữ liệu khiến một nhà thực nghiệm khó có thể bác bỏ các lý thuyết của chính mình. Mặt khác, các nhà sinh vật học nghiên cứu hành vi động vật lại thiếu dữ liệu nội quan và đã phát triển một phương pháp luận khách quan, như được mô tả bởi H. S. Jennings (1906) trong tác phẩm có ảnh hưởng của ông *Behavior of the Lower Organisms* (Hành vi của các sinh vật bậc thấp). Áp dụng quan điểm này cho con người, phong trào thuyết hành vi (behaviorism), do John Watson (1878-1958) lãnh đạo, đã bác bỏ bất kỳ lý thuyết nào liên quan đến các quá trình tâm thần với lý do rằng nội quan không thể cung cấp bằng chứng đáng tin cậy. Những người theo thuyết hành vi nhấn mạnh vào việc chỉ nghiên cứu các thước đo khách quan của các tri giác (hoặc kích thích) được đưa cho động vật và các hành động (hoặc phản ứng) kết quả của nó. Thuyết hành vi đã khám phá ra rất nhiều điều về chuột và chim bồ câu nhưng ít thành công hơn trong việc hiểu con người.

Tâm lý học nhận thức (Cognitive psychology), coi bộ não như một thiết bị xử lý thông tin, có thể được truy tìm ít nhất từ các tác phẩm của William James (1842-1910). Helmholtz cũng nhấn mạnh rằng tri giác liên quan đến một dạng suy luận logic vô thức. Quan điểm nhận thức phần lớn bị thuyết hành vi làm lu mờ ở Hoa Kỳ, nhưng tại Đơn vị Tâm lý học Ứng dụng của Cambridge, do Frederic Bartlett (1886–1969) chỉ đạo, mô hình hóa nhận thức đã có thể phát triển mạnh. Cuốn *The Nature of Explanation* (Bản chất của sự giải thích), của học trò và người kế nhiệm của Bartlett là Kenneth Craik (1943), đã mạnh mẽ tái lập tính hợp pháp của các thuật ngữ "tâm thần" như niềm tin và mục tiêu, cho rằng chúng cũng khoa học như, chẳng hạn, việc sử dụng áp suất và nhiệt độ để nói về khí, mặc dù chúng được tạo thành từ các phân tử không có cả hai. Craik đã chỉ định ba bước chính của một tác nhân dựa trên tri thức: (1) kích thích phải được chuyển thành một biểu diễn bên trong, (2) biểu diễn đó được thao tác bởi các quá trình nhận thức để rút ra các biểu diễn bên trong mới, và (3) những biểu diễn này lần lượt được chuyển ngược lại thành hành động. Ông đã rõ ràng... (câu bị cắt).

### 1.2.6 Kỹ thuật máy tính (Computer engineering)

*   Làm thế nào chúng ta có thể xây dựng một máy tính hiệu quả?

Để trí tuệ nhân tạo thành công, chúng ta cần hai thứ: trí thông minh và một tạo tác. Máy tính đã là tạo tác được lựa chọn. Máy tính điện tử kỹ thuật số hiện đại được phát minh độc lập và gần như đồng thời bởi các nhà khoa học ở ba quốc gia đang tham chiến trong Thế chiến II. Máy tính hoạt động đầu tiên là Heath Robinson cơ điện, được chế tạo vào năm 1940 bởi nhóm của Alan Turing cho một mục đích duy nhất: giải mã tin nhắn của Đức. Năm 1943, cùng nhóm này đã phát triển Colossus, một cỗ máy đa năng mạnh mẽ dựa trên đèn điện tử chân không. Máy tính có thể lập trình hoạt động đầu tiên là Z-3, phát minh của Konrad Zuse ở Đức vào năm 1941. Zuse cũng phát minh ra số dấu phẩy động và ngôn ngữ lập trình bậc cao đầu tiên, Plankalkül. Máy tính điện tử đầu tiên, ABC, được lắp ráp bởi John Atanasoff và sinh viên của ông là Clifford Berry từ năm 1940 đến 1942 tại Đại học bang Iowa. Nghiên cứu của Atanasoff ít được hỗ trợ hoặc công nhận; chính ENIAC, được phát triển như một phần của một dự án quân sự bí mật tại Đại học Pennsylvania bởi một nhóm bao gồm John Mauchly và John Eckert, mới chứng tỏ là tiền thân có ảnh hưởng nhất của các máy tính hiện đại.

---

## LỊCH SỬ CỦA AI

### 1.3.1 Sự hình thành của trí tuệ nhân tạo (1943-1955)

Công trình đầu tiên hiện được công nhận rộng rãi là AI được thực hiện bởi Warren McCulloch và Walter Pitts (1943). Họ dựa trên ba nguồn: kiến thức về sinh lý học cơ bản và chức năng của các neuron trong não; một phân tích hình thức về logic mệnh đề của Russell và Whitehead; và lý thuyết tính toán của Turing. Họ đề xuất một mô hình neuron nhân tạo trong đó mỗi neuron được đặc trưng là "bật" hoặc "tắt", với việc chuyển sang "bật" xảy ra để đáp ứng với sự kích thích bởi một số lượng đủ các neuron lân cận. Trạng thái của một neuron được coi là "tương đương về mặt thực tế với một mệnh đề đề xuất kích thích thích hợp của nó." Họ đã chỉ ra, ví dụ, rằng bất kỳ hàm nào có thể tính toán được đều có thể được tính toán bởi một mạng lưới các neuron được kết nối nào đó, và tất cả các kết nối logic (và, hoặc, không, v.v.) đều có thể được thực hiện bằng các cấu trúc mạng đơn giản. McCulloch và Pitts cũng đề xuất rằng các mạng được xác định phù hợp có thể học. Donald Hebb (1949) đã chứng minh một quy tắc cập nhật đơn giản để sửa đổi độ mạnh kết nối giữa các neuron. Quy tắc của ông, ngày nay được gọi là học Hebbian (Hebbian learning), vẫn là một mô hình có ảnh hưởng cho đến ngày nay.

Hai sinh viên đại học tại Harvard, Marvin Minsky và Dean Edmonds, đã chế tạo máy tính mạng nơ-ron đầu tiên vào năm 1950. SNARC, tên gọi của nó, đã sử dụng 3000 ống chân không và một cơ chế lái tự động dư thừa từ máy bay ném bom B-24 để mô phỏng một mạng lưới 40 nơ-ron. Sau đó, tại Princeton, Minsky đã nghiên cứu tính toán phổ quát trong các mạng nơ-ron. Hội đồng Tiến sĩ của ông đã hoài nghi về việc liệu loại công việc này có nên được coi là toán học hay không, nhưng von Neumann được cho là đã nói: "Nếu bây giờ không phải là nó, thì một ngày nào đó nó sẽ là." Minsky sau này đã chứng minh các định lý có ảnh hưởng cho thấy những hạn chế của nghiên cứu mạng nơ-ron.

### 1.3.2 Sự ra đời của trí tuệ nhân tạo (1956)

Princeton là nơi có một nhân vật có ảnh hưởng khác trong AI, John McCarthy. Sau khi nhận bằng Tiến sĩ ở đó vào năm 1951 và làm giảng viên trong hai năm, McCarthy chuyển đến Stanford và sau đó đến Dartmouth College, nơi đã trở thành nơi khai sinh chính thức của lĩnh vực này. McCarthy đã thuyết phục Minsky, Claude Shannon và Nathaniel Rochester giúp ông tập hợp các nhà nghiên cứu Hoa Kỳ quan tâm đến lý thuyết automata, mạng nơ-ron và nghiên cứu về trí thông minh. Họ đã tổ chức một hội thảo kéo dài hai tháng tại Dartmouth vào mùa hè năm 1956. Đề xuất nêu rõ:

> Chúng tôi đề xuất một nghiên cứu kéo dài 2 tháng, với 10 người về trí tuệ nhân tạo sẽ được thực hiện vào mùa hè năm 1956 tại Dartmouth College ở Hanover, New Hampshire. Nghiên cứu sẽ tiến hành dựa trên giả thuyết rằng mọi khía cạnh của học tập hoặc bất kỳ đặc điểm nào khác của trí thông minh về nguyên tắc đều có thể được mô tả một cách chính xác đến mức một cỗ máy có thể được tạo ra để mô phỏng nó. Sẽ có một nỗ lực để tìm cách làm cho máy móc sử dụng ngôn ngữ, hình thành các khái niệm và trừu tượng hóa, giải quyết các loại vấn đề hiện đang dành riêng cho con người và tự cải thiện. Chúng tôi nghĩ rằng một bước tiến đáng kể có thể được thực hiện trong một hoặc nhiều vấn đề này nếu một nhóm các nhà khoa học được lựa chọn cẩn thận cùng làm việc với nó trong một mùa hè.

### 1.3.3 Sự nhiệt tình ban đầu, kỳ vọng lớn (1952-1969)

Những năm đầu của AI đầy những thành công—theo một cách hạn chế. Với các máy tính và công cụ lập trình thô sơ thời đó và thực tế là chỉ vài năm trước đó máy tính được coi là những thứ chỉ có thể làm số học và không hơn thế nữa, thật đáng kinh ngạc mỗi khi một máy tính làm được điều gì đó dù chỉ là thông minh một cách mơ hồ. Giới trí thức, nói chung, thích tin rằng "một cỗ máy không bao giờ có thể làm được X." (Xem Chương 26 để biết danh sách dài các X được Turing thu thập.) Các nhà nghiên cứu AI tự nhiên đã đáp lại bằng cách chứng minh hết X này đến X khác. John McCarthy gọi giai đoạn này là kỷ nguyên "Nhìn kìa mẹ, không cần tay!"

Thành công ban đầu của Newell và Simon được tiếp nối bằng General Problem Solver (Bộ giải quyết vấn đề tổng quát), hay GPS. Không giống như Logic Theorist, chương trình này được thiết kế ngay từ đầu để bắt chước các giao thức giải quyết vấn đề của con người. Trong lớp câu đố hạn chế mà nó có thể xử lý, hóa ra thứ tự mà chương trình xem xét các mục tiêu phụ và các hành động có thể xảy ra tương tự như cách con người tiếp cận các vấn đề tương tự. Do đó, GPS có lẽ là chương trình đầu tiên thể hiện cách tiếp cận "suy nghĩ như con người". Thành công của GPS và các chương trình tiếp theo như các mô hình nhận thức đã khiến Newell và Simon (1976) xây dựng giả thuyết hệ thống ký hiệu vật lý (physical symbol system hypothesis) nổi tiếng, phát biểu rằng "một hệ thống ký hiệu vật lý có các phương tiện cần thiết và đủ cho hành động thông minh nói chung." Ý của họ là bất kỳ hệ thống nào (con người hoặc máy móc) thể hiện trí thông minh đều phải hoạt động bằng cách thao tác các cấu trúc dữ liệu bao gồm các ký hiệu. Chúng ta sẽ thấy sau này rằng giả thuyết này đã bị thách thức từ nhiều hướng.

Tại IBM, Nathaniel Rochester và các đồng nghiệp của ông đã tạo ra một số chương trình AI đầu tiên. Herbert Gelernter (1959) đã xây dựng Geometry Theorem Prover (Bộ chứng minh định lý hình học), có khả năng chứng minh các định lý mà nhiều sinh viên toán học sẽ thấy khá phức tạp. Bắt đầu từ năm 1952, Arthur Samuel đã viết một loạt chương trình chơi cờ đam (draughts) mà cuối cùng đã học được cách chơi ở cấp độ nghiệp dư mạnh. Trên đường đi, ông đã bác bỏ ý tưởng rằng máy tính chỉ có thể làm những gì chúng được bảo: chương trình của ông nhanh chóng học được cách chơi một ván cờ hay hơn người tạo ra nó. Chương trình đã được trình diễn trên truyền hình vào tháng 2 năm 1956, tạo ấn tượng mạnh. Giống như Turing, Samuel gặp khó khăn trong việc tìm thời gian sử dụng máy tính. Làm việc vào ban đêm, ông đã sử dụng những chiếc máy vẫn còn đang trong giai đoạn thử nghiệm tại nhà máy sản xuất của IBM. Chương 5 bao gồm việc chơi game, và Chương 21 giải thích các kỹ thuật học tập được Samuel sử dụng.

John McCarthy chuyển từ Dartmouth đến MIT và ở đó đã có ba đóng góp quan trọng trong một năm lịch sử: 1958. Trong Bản ghi nhớ số 1 của Phòng thí nghiệm AI MIT, McCarthy đã định nghĩa ngôn ngữ cấp cao Lisp, ngôn ngữ này đã trở thành ngôn ngữ lập trình AI thống trị trong 30 năm tới. Với Lisp, McCarthy đã có công cụ mình cần, nhưng việc truy cập vào các tài nguyên máy tính khan hiếm và đắt đỏ cũng là một vấn đề nghiêm trọng. Để đối phó, ông và những người khác tại MIT đã phát minh ra chế độ chia sẻ thời gian (time sharing). Cũng trong năm 1958, McCarthy đã xuất bản một bài báo có tựa đề *Programs with Common Sense* (Các chương trình có tri thức thông thường), trong đó ông mô tả Advice Taker, một chương trình giả định có thể được coi là hệ thống AI hoàn chỉnh đầu tiên. Giống như Logic Theorist và Geometry Theorem Prover, chương trình của McCarthy được thiết kế để sử dụng tri thức để tìm kiếm giải pháp cho các vấn đề. Nhưng không giống như những chương trình khác, nó nhằm mục đích thể hiện tri thức chung về thế giới. Ví dụ, ông đã chỉ ra làm thế nào một số tiên đề đơn giản sẽ cho phép chương trình tạo ra một kế hoạch để lái xe đến sân bay. Chương trình cũng được thiết kế để chấp nhận các tiên đề mới trong quá trình hoạt động bình thường, do đó cho phép nó đạt được năng lực trong các lĩnh vực mới mà không cần lập trình lại. Do đó, Advice Taker đã thể hiện các nguyên tắc trung tâm của biểu diễn tri thức và suy luận: rằng việc có một biểu diễn hình thức, rõ ràng về thế giới và các hoạt động của nó và có thể thao tác biểu diễn đó bằng các quy trình suy diễn là hữu ích. Điều đáng chú ý là phần lớn bài báo năm 1958 vẫn còn phù hợp cho đến ngày nay.

Công trình ban đầu xây dựng trên các mạng nơ-ron của McCulloch và Pitts cũng phát triển mạnh mẽ. Công trình của Winograd và Cowan (1963) cho thấy làm thế nào một số lượng lớn các yếu tố có thể cùng nhau đại diện cho một khái niệm riêng lẻ, với sự gia tăng tương ứng về tính mạnh mẽ và tính song song. Các phương pháp học của Hebb được tăng cường bởi Bernie Widrow (Widrow và Hoff, 1960; Widrow, 1962), người gọi mạng của mình là adalines, và bởi Frank Rosenblatt (1962) với perceptron của ông. Định lý hội tụ perceptron (perceptron convergence theorem) (Block et al., 1962) nói rằng thuật toán học có thể điều chỉnh độ mạnh kết nối của một perceptron để khớp với bất kỳ dữ liệu đầu vào nào, miễn là sự khớp đó tồn tại. Các chủ đề này được đề cập trong Chương 20.

Lĩnh vực nhận dạng giọng nói minh họa cho mô hình này. Trong những năm 1970, rất nhiều kiến trúc và cách tiếp cận khác nhau đã được thử nghiệm. Nhiều trong số này khá đặc thù (ad hoc) và mong manh, và chỉ được trình diễn trên một vài ví dụ được chọn đặc biệt. Trong những năm gần đây, các phương pháp dựa trên mô hình Markov ẩn (HMMs - hidden Markov models) đã chiếm ưu thế trong lĩnh vực này. Hai khía cạnh của HMM là có liên quan. Thứ nhất, chúng dựa trên một lý thuyết toán học chặt chẽ. Điều này đã cho phép các nhà nghiên cứu giọng nói xây dựng dựa trên nhiều thập kỷ kết quả toán học được phát triển trong các lĩnh vực khác. Thứ hai, chúng được tạo ra bằng một quy trình huấn luyện trên một kho ngữ liệu lớn dữ liệu giọng nói thực. Điều này đảm bảo rằng hiệu suất là mạnh mẽ, và trong các thử nghiệm mù nghiêm ngặt, các HMM đã cải thiện đều đặn điểm số của chúng. Công nghệ giọng nói và lĩnh vực liên quan là nhận dạng ký tự viết tay đã và đang chuyển sang các ứng dụng công nghiệp và tiêu dùng rộng rãi. Lưu ý rằng không có tuyên bố khoa học nào cho rằng con người sử dụng HMM để nhận dạng giọng nói; thay vào đó, HMM cung cấp một khuôn khổ toán học để hiểu vấn đề và hỗ trợ tuyên bố kỹ thuật rằng chúng hoạt động tốt trong thực tế.

Dịch máy (Machine translation) theo cùng một hướng với nhận dạng giọng nói. Trong những năm 1950, đã có sự nhiệt tình ban đầu đối với một cách tiếp cận dựa trên các chuỗi từ, với các mô hình được học theo các nguyên tắc của lý thuyết thông tin. Cách tiếp cận đó không còn được ưa chuộng trong những năm 1960, nhưng đã quay trở lại vào cuối những năm 1990 và hiện đang thống trị lĩnh vực này.

Mạng nơ-ron cũng phù hợp với xu hướng này. Phần lớn công việc về mạng nơ-ron trong những năm 1980 được thực hiện nhằm cố gắng xác định phạm vi những gì có thể làm được và tìm hiểu xem mạng nơ-ron khác với các kỹ thuật "truyền thống" như thế nào. Sử dụng phương pháp luận cải tiến và các khuôn khổ lý thuyết, lĩnh vực này đã đạt được sự hiểu biết trong đó mạng nơ-ron hiện có thể được so sánh với các kỹ thuật tương ứng từ thống kê, nhận dạng mẫu và học máy, và kỹ thuật hứa hẹn nhất có thể được áp dụng cho từng ứng dụng. Kết quả của những phát triển này, công nghệ được gọi là khai phá dữ liệu (data mining) đã tạo ra một ngành công nghiệp mới mạnh mẽ.

---

## TÁC NHÂN THÔNG MINH (INTELLIGENT AGENT)

Chương 1 đã xác định khái niệm tác nhân hợp lý (rational agents) là trung tâm trong cách tiếp cận của chúng ta đối với trí tuệ nhân tạo. Trong chương này, chúng ta làm cho khái niệm này trở nên cụ thể hơn. Chúng ta sẽ thấy rằng khái niệm về tính hợp lý có thể được áp dụng cho nhiều loại tác nhân hoạt động trong bất kỳ môi trường nào có thể tưởng tượng được. Kế hoạch của chúng ta trong cuốn sách này là sử dụng khái niệm này để phát triển một tập hợp nhỏ các nguyên tắc thiết kế để xây dựng các tác nhân thành công—các hệ thống có thể được gọi một cách hợp lý là thông minh.

Chúng ta bắt đầu bằng cách kiểm tra các tác nhân, môi trường và sự kết hợp giữa chúng. Quan sát rằng một số tác nhân hoạt động tốt hơn những tác nhân khác dẫn đến một cách tự nhiên ý tưởng về một tác nhân hợp lý—một tác nhân hoạt động tốt nhất có thể. Một tác nhân có thể hoạt động tốt đến mức nào phụ thuộc vào bản chất của môi trường; một số môi trường khó khăn hơn những môi trường khác. Chúng ta đưa ra một cách phân loại sơ bộ các môi trường và chỉ ra các đặc tính của một môi trường ảnh hưởng đến việc thiết kế các tác nhân phù hợp cho môi trường đó như thế nào. Chúng ta mô tả một số thiết kế tác nhân "khung sườn" cơ bản, mà chúng ta sẽ cụ thể hóa trong phần còn lại của cuốn sách.

Một **tác nhân (agent)** là bất cứ thứ gì có thể được xem là nhận thức môi trường của nó thông qua các **cảm biến (sensors)** và hành động lên môi trường đó thông qua các **bộ truyền động (actuators)**. Ý tưởng đơn giản này được minh họa trong Hình 2.1 (không được cung cấp trong OCR). Một tác nhân con người có mắt, tai và các cơ quan khác làm cảm biến và tay, chân, bộ máy phát âm, v.v. làm bộ truyền động. Một tác nhân robot có thể có camera và máy đo khoảng cách hồng ngoại làm cảm biến và các động cơ khác nhau làm bộ truyền động. Một tác nhân phần mềm nhận các phím bấm, nội dung tệp và các gói mạng làm đầu vào cảm giác và hành động lên môi trường bằng cách hiển thị trên màn hình, ghi tệp và gửi các gói mạng.

Chúng ta sử dụng thuật ngữ **nhận thức (percept)** để chỉ các đầu vào tri giác của tác nhân tại bất kỳ thời điểm nào. **Chuỗi nhận thức (percept sequence)** của một tác nhân là lịch sử hoàn chỉnh của mọi thứ mà tác nhân đã từng nhận thức được. Nói chung, lựa chọn hành động của một tác nhân tại bất kỳ thời điểm nào có thể phụ thuộc vào toàn bộ chuỗi nhận thức được quan sát cho đến nay, nhưng không phụ thuộc vào bất cứ điều gì nó chưa nhận thức được. Bằng cách chỉ định lựa chọn hành động của tác nhân cho mọi chuỗi nhận thức có thể có, chúng ta đã nói ít nhiều mọi thứ cần nói về tác nhân đó. Về mặt toán học, chúng ta nói rằng hành vi của một tác nhân được mô tả bởi **hàm tác nhân (agent function)** ánh xạ bất kỳ chuỗi nhận thức nào đã cho thành một hành động.

---

## KHÁI NIỆM VỀ TÍNH HỢP LÝ (THE CONCEPT OF RATIONALITY)

Một **tác nhân hợp lý (rational agent)** là một tác nhân làm điều đúng đắn—nói một cách khái niệm, mọi mục trong bảng cho hàm tác nhân đều được điền một cách chính xác. Rõ ràng, làm điều đúng đắn tốt hơn là làm điều sai trái, nhưng làm điều đúng đắn có nghĩa là gì?

Chúng ta trả lời câu hỏi lâu đời này theo một cách lâu đời: bằng cách xem xét hậu quả của hành vi của tác nhân. Khi một tác nhân được đặt vào một môi trường, nó tạo ra một chuỗi các hành động theo các nhận thức mà nó nhận được. Chuỗi hành động này khiến môi trường trải qua một chuỗi các trạng thái. Nếu chuỗi này là mong muốn, thì tác nhân đã hoạt động tốt. Khái niệm về tính mong muốn này được nắm bắt bởi một **thước đo hiệu suất (performance measure)** đánh giá bất kỳ chuỗi trạng thái môi trường nào đã cho.

Lưu ý rằng chúng ta đã nói *trạng thái môi trường*, chứ không phải *trạng thái tác nhân*. Nếu chúng ta định nghĩa thành công dựa trên ý kiến của tác nhân về hiệu suất của chính nó, một tác nhân có thể đạt được tính hợp lý hoàn hảo đơn giản bằng cách tự lừa dối mình rằng hiệu suất của nó là hoàn hảo. Các tác nhân con người đặc biệt nổi tiếng với "nho còn xanh"—tin rằng họ không thực sự muốn một thứ gì đó (ví dụ: giải Nobel) sau khi không đạt được nó.

### 2.2.1 Tính hợp lý (Rationality)

Điều gì là hợp lý tại bất kỳ thời điểm nào phụ thuộc vào bốn yếu tố:

*   Thước đo hiệu suất xác định tiêu chí thành công.
*   Kiến thức tiên nghiệm của tác nhân về môi trường.
*   Các hành động mà tác nhân có thể thực hiện.
*   Chuỗi nhận thức của tác nhân cho đến nay.

Điều này dẫn đến một định nghĩa về tác nhân hợp lý:

> Đối với mỗi chuỗi nhận thức có thể có, một tác nhân hợp lý nên chọn một hành động được kỳ vọng sẽ tối đa hóa thước đo hiệu suất của nó, dựa trên bằng chứng được cung cấp bởi chuỗi nhận thức và bất kỳ kiến thức tích hợp nào mà tác nhân có.

### 2.2.2 Toàn tri, học tập và tự chủ (Omniscience, learning, and autonomy)

Chúng ta cần cẩn thận để phân biệt giữa tính hợp lý và **toàn tri (omniscience)**. Một tác nhân toàn tri biết kết quả thực tế của các hành động của nó và có thể hành động tương ứng; nhưng toàn tri là không thể trong thực tế. Hãy xem xét ví dụ sau: Tôi đang đi dạo trên đại lộ Champs Elysées một ngày nọ và tôi thấy một người bạn cũ ở bên kia đường. Gần đó không có xe cộ và tôi cũng không bận việc gì khác, vì vậy, là một người hợp lý, tôi bắt đầu băng qua đường. Trong khi đó, ở độ cao 33.000 feet, một cánh cửa chở hàng rơi ra khỏi một chiếc máy bay chở hàng đang bay qua, và trước khi tôi qua được bên kia đường, tôi bị đè bẹp. Tôi có phi lý khi băng qua đường không? Không chắc cáo phó của tôi sẽ ghi "Kẻ ngốc cố gắng băng qua đường."

Ví dụ này cho thấy tính hợp lý không giống như sự hoàn hảo. Tính hợp lý tối đa hóa hiệu suất *kỳ vọng*, trong khi sự hoàn hảo tối đa hóa hiệu suất *thực tế*. Việc rút lui khỏi yêu cầu về sự hoàn hảo không chỉ là vấn đề công bằng với các tác nhân. Vấn đề là nếu chúng ta mong đợi một tác nhân làm điều hóa ra là hành động tốt nhất sau khi sự việc đã xảy ra, thì sẽ không thể thiết kế một tác nhân để đáp ứng đặc tả này—trừ khi chúng ta cải thiện hiệu suất của quả cầu pha lê hoặc máy thời gian.

Định nghĩa của chúng ta về tính hợp lý không yêu cầu sự toàn tri, bởi vì lựa chọn hợp lý chỉ phụ thuộc vào chuỗi nhận thức *cho đến nay*. Chúng ta cũng phải đảm bảo rằng chúng ta không vô tình cho phép tác nhân tham gia vào các hoạt động rõ ràng là kém thông minh. Ví dụ, nếu một tác nhân không nhìn cả hai bên trước khi băng qua một con đường đông đúc, thì chuỗi nhận thức của nó sẽ không cho nó biết rằng có một chiếc xe tải lớn đang lao tới với tốc độ cao. Định nghĩa của chúng ta về tính hợp lý có nói rằng bây giờ việc băng qua đường là ổn không? Hoàn toàn không! Thứ nhất, sẽ không hợp lý khi băng qua đường với chuỗi nhận thức không cung cấp thông tin này: nguy cơ tai nạn khi băng qua đường mà không nhìn là quá lớn. Thứ hai, một tác nhân hợp lý nên chọn hành động "nhìn" trước khi bước xuống đường, bởi vì việc nhìn giúp tối đa hóa hiệu suất kỳ vọng. Thực hiện các hành động để sửa đổi các nhận thức trong tương lai—đôi khi được gọi là **thu thập thông tin (information gathering)**—là một phần quan trọng của tính hợp lý và được đề cập sâu trong Chương 16. Một ví dụ thứ hai về thu thập thông tin được cung cấp bởi việc **khám phá (exploration)** mà một tác nhân hút bụi phải thực hiện trong một môi trường ban đầu chưa biết.

Định nghĩa của chúng ta yêu cầu một tác nhân hợp lý không chỉ thu thập thông tin mà còn phải **học (learn)** càng nhiều càng tốt từ những gì nó nhận thức được. Cấu hình ban đầu của tác nhân có thể phản ánh một số kiến thức tiên nghiệm về môi trường, nhưng khi tác nhân có thêm kinh nghiệm, kiến thức này có thể được sửa đổi và bổ sung. Có những trường hợp cực đoan trong đó môi trường hoàn toàn được biết *a priori*. Trong những trường hợp như vậy, tác nhân không cần phải nhận thức hay học hỏi; nó chỉ đơn giản là hành động một cách chính xác. Tất nhiên, những tác nhân như vậy rất mong manh. Hãy xem xét con bọ hung tầm thường. Sau khi đào tổ và đẻ trứng, nó lấy một viên phân từ một đống gần đó để bịt lối vào. Nếu viên phân bị lấy đi khỏi tay nó trên đường đi, con bọ vẫn tiếp tục nhiệm vụ của mình và thực hiện các động tác bịt tổ bằng viên phân không tồn tại, không bao giờ nhận thấy rằng nó đã mất. Sự tiến hóa đã xây dựng một giả định vào hành vi của con bọ, và khi nó bị vi phạm, kết quả là hành vi không thành công. Thông minh hơn một chút là ong bắp cày sphex. Con cái sphex sẽ đào một cái hang, đi ra ngoài đốt một con sâu bướm và kéo nó đến hang, vào lại hang để kiểm tra xem mọi thứ có ổn không, kéo con sâu bướm vào trong và đẻ trứng. Con sâu bướm đóng vai trò là nguồn thức ăn khi trứng nở. Cho đến nay vẫn ổn, nhưng nếu một nhà côn trùng học di chuyển con sâu bướm đi vài inch trong khi con sphex đang kiểm tra, nó sẽ quay lại bước "kéo" trong kế hoạch của mình và sẽ tiếp tục kế hoạch mà không sửa đổi, ngay cả sau hàng chục lần can thiệp di chuyển sâu bướm. Con sphex không thể học được rằng kế hoạch bẩm sinh của nó đang thất bại, và do đó sẽ không thay đổi nó.

Trong chừng mực một tác nhân dựa vào kiến thức tiên nghiệm của người thiết kế thay vì dựa vào nhận thức của chính nó, chúng ta nói rằng tác nhân đó thiếu **tính tự chủ (autonomy)**. Một tác nhân hợp lý nên tự chủ—nó nên học những gì có thể để bù đắp cho kiến thức tiên nghiệm một phần hoặc không chính xác. Ví dụ, một tác nhân hút bụi học cách dự đoán vị trí và thời điểm bụi bẩn bổ sung sẽ xuất hiện sẽ hoạt động tốt hơn một tác nhân không làm được điều đó. Về mặt thực tế, người ta hiếm khi yêu cầu sự tự chủ hoàn toàn ngay từ đầu: khi tác nhân có ít hoặc không có kinh nghiệm, nó sẽ phải hành động ngẫu nhiên trừ khi người thiết kế hỗ trợ. Vì vậy, cũng giống như quá trình tiến hóa cung cấp cho động vật đủ phản xạ bẩm sinh để tồn tại đủ lâu để tự học, việc cung cấp cho một tác nhân thông minh nhân tạo một số kiến thức ban đầu cũng như khả năng học hỏi là hợp lý. Sau khi có đủ kinh nghiệm về môi trường của mình, hành vi của một tác nhân hợp lý có thể trở nên độc lập một cách hiệu quả với kiến thức tiên nghiệm của nó. Do đó, việc kết hợp học tập cho phép người ta thiết kế một tác nhân hợp lý duy nhất sẽ thành công trong nhiều loại môi trường khác nhau.

### 2.3.1 Chỉ định môi trường tác vụ (Specifying the task environment)

Bây giờ chúng ta đã có một định nghĩa về tính hợp lý, chúng ta gần như sẵn sàng để nghĩ về việc xây dựng các tác nhân hợp lý. Tuy nhiên, trước tiên, chúng ta phải suy nghĩ về **môi trường tác vụ (task environments)**, về cơ bản là "vấn đề" mà các tác nhân hợp lý là "giải pháp". Chúng ta bắt đầu bằng cách chỉ ra cách chỉ định một môi trường tác vụ, minh họa quy trình bằng một số ví dụ. Sau đó, chúng ta chỉ ra rằng các môi trường tác vụ có nhiều dạng khác nhau. Dạng của môi trường tác vụ ảnh hưởng trực tiếp đến thiết kế phù hợp cho chương trình tác nhân.

Trong cuộc thảo luận của chúng ta về tính hợp lý của tác nhân hút bụi đơn giản, chúng ta đã phải chỉ định thước đo hiệu suất, môi trường, cũng như bộ truyền động và cảm biến của tác nhân. Chúng ta nhóm tất cả những điều này dưới tiêu đề môi trường tác vụ. Đối với những người thích từ viết tắt, chúng ta gọi đây là mô tả **PEAS (Performance, Environment, Actuators, Sensors)**. Khi thiết kế một tác nhân, bước đầu tiên phải luôn là chỉ định môi trường tác vụ một cách đầy đủ nhất có thể.

**Hình 2.4 Mô tả PEAS (Hiệu suất, Môi trường, Bộ truyền động, Cảm biến) của môi trường tác vụ cho một chiếc taxi tự động.**

| Loại tác nhân (Agent Type) | Thước đo hiệu suất (Performance Measure)                                  | Môi trường (Environment)                                            | Bộ truyền động (Actuators)                                | Cảm biến (Sensors)                                                                                                 |
| :------------------------- | :----------------------------------------------------------------------- | :------------------------------------------------------------------- | :-------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |
| Tài xế taxi (Taxi driver)  | Chuyến đi an toàn, nhanh, hợp pháp, thoải mái, tối đa hóa lợi nhuận        | Đường sá, giao thông khác, người đi bộ, khách hàng                    | Vô lăng, chân ga, phanh, xi nhan, còi, màn hình hiển thị    | Camera, sonar, đồng hồ tốc độ, GPS, đồng hồ đo quãng đường, gia tốc kế, cảm biến động cơ, bàn phím                     |

Trước tiên, thước đo hiệu suất mà chúng ta muốn người lái xe tự động của mình hướng tới là gì? Các phẩm chất mong muốn bao gồm đến đúng điểm đến; giảm thiểu mức tiêu thụ nhiên liệu và hao mòn; giảm thiểu thời gian hoặc chi phí chuyến đi; giảm thiểu vi phạm luật giao thông và gây phiền nhiễu cho các tài xế khác; tối đa hóa sự an toàn và thoải mái của hành khách; tối đa hóa lợi nhuận. Rõ ràng, một số mục tiêu này mâu thuẫn với nhau, vì vậy sẽ cần phải có sự đánh đổi.

Tiếp theo, môi trường lái xe mà chiếc taxi sẽ phải đối mặt là gì? Bất kỳ tài xế taxi nào cũng phải đối phó với nhiều loại đường khác nhau, từ những con đường nông thôn và ngõ hẻm trong thành phố đến những đường cao tốc 12 làn xe. Đường sá có các phương tiện giao thông khác, người đi bộ, động vật đi lạc, công trình đường bộ, xe cảnh sát, vũng nước và ổ gà. Chiếc taxi cũng phải tương tác với hành khách tiềm năng và thực tế. Cũng có một số lựa chọn tùy chọn. Chiếc taxi có thể cần hoạt động ở Nam California, nơi tuyết hiếm khi là vấn đề, hoặc ở Alaska, nơi tuyết hiếm khi không phải là vấn đề. Nó có thể luôn lái xe bên phải, hoặc chúng ta có thể muốn nó đủ linh hoạt để lái xe bên trái khi ở Anh hoặc Nhật Bản. Rõ ràng, môi trường càng hạn chế, vấn đề thiết kế càng dễ dàng.

Các bộ truyền động cho một chiếc taxi tự động bao gồm những bộ phận có sẵn cho một người lái xe: điều khiển động cơ thông qua chân ga và điều khiển việc lái và phanh. Ngoài ra, nó sẽ cần đầu ra cho màn hình hiển thị hoặc bộ tổng hợp giọng nói để nói chuyện lại với hành khách, và có lẽ một cách nào đó để giao tiếp với các phương tiện khác, một cách lịch sự hoặc theo cách khác.

Các cảm biến cơ bản cho chiếc taxi sẽ bao gồm một hoặc nhiều camera video có thể điều khiển để nó có thể nhìn thấy đường; nó có thể tăng cường chúng bằng các cảm biến hồng ngoại hoặc sonar để phát hiện khoảng cách đến các xe khác và chướng ngại vật. Để tránh bị phạt vì chạy quá tốc độ, chiếc taxi nên có đồng hồ tốc độ, và để điều khiển xe đúng cách, đặc biệt là ở những đoạn cua, nó nên có gia tốc kế. Để xác định trạng thái cơ học của xe, nó sẽ cần dãy cảm biến động cơ, nhiên liệu và hệ thống điện thông thường. Giống như nhiều người lái xe, nó có thể muốn một hệ thống định vị toàn cầu (GPS) để không bị lạc. Cuối cùng, nó sẽ cần một bàn phím hoặc micrô để hành khách yêu cầu điểm đến.

<!-- Kết thúc -->
*Được Việt hoá từ tài liệu của TS. Lê Quang Minh cung cấp.*